---
title: "Investigating Indie"
output: 
  flexdashboard::flex_dashboard:
    theme: journal
---

```{r setup, include=FALSE}
library(flexdashboard)
library(tidyverse)
library(ggplot2)
library(spotifyr)
library(compmus)
library(plotly)
library(gridExtra)
library(dplyr)
library(tidymodels)
library(ggdendro)
library(heatmaply)

folk <- get_playlist_audio_features("", "6P82OCSEpo1VrZ4TZ2W5jK")
pop <- get_playlist_audio_features("", "3HDdTtyB76aLDc4wpGFCX4")
rock <- get_playlist_audio_features("","5dMoFfrgm8aOOkneQjt6LT")

indie <-
  bind_rows(
    folk |> mutate(category = "Indie Folk"),
    pop |> mutate(category = "Indie Pop") |> slice_head(n = 50),
    rock |> mutate(category= "Indie Rock")
  ) |> add_audio_analysis()
```


Introduction
===================================== 

### What makes a song "Indie"? 
There is an ongoing debate on whether indie music can be considered a genre. Indie is short for “independent” and refers to music released by an artist independently rather than with a record label. Some bands that are considered to be indie, such as Arcade Fire, however, work with a record label. This begs the question: What makes indie music indie? Some argue that indie music has a specific sound, but what are its defining characteristics that allows Spotify to create “Indie” playlists? I hope to investigate this question with my corpus containing songs from three different indie sub-genre playlists created by Spotify: “Indie Rock Mix,” “Indie Pop Mix,” and “Indie Folk Mix.” Comparing songs from these different sub-genres will clarify what characteristics of a song Spotify considers to be indie. The main limitation of my investigation is that the songs in the playlists are recommended by Spotify for my specific account. Hence, it may not reflect the most popular indie songs nor be representative of indie music as a whole. “The Less I Know the Better” by Tame Impala is a typical indie song in my corpus, because it cannot be tied to another genre. Two atypical songs are “Sunflower” by Harry Styles and “Work Song” by Hozier, because neither of these artists are considered to be indie artists. These songs will be good to analyze since they may have the characteristics of a typical indie song, but they are made by non-indie artists.
  
Indie folk combines the catchy melodies of indie rock with the acoustical sounds of contemporary folk music.
  
  
```{r}

```


Comparing the Three Playlists {.storyboard}
===========================================================

### Indie Rock is Energetic {data-commentary-width=400}
```{r}
dens_energy <- ggplot(indie, aes(x = category, y=energy, color = category)) + geom_boxplot() +
  labs(x = "Energy", y = "Density", 
       title = "Comparing the Distribution of Energy Between Indie Subgenres") 
ggplotly(dens_energy)
```

*** 
Energy is a Spotify track-level feature measured between 0 to 1, with 0 being the lowest energy and 1 the highest. This boxplot shows that energy is widespread over the Indie genre as a whole. A song's energy cannot tell us whether it belongs under the general "Indie" label. What it does show, however, is a difference in energy between the three playlists. Spotify's "Indie Rock" playlist contains the songs with the most energy, with a mean energy of 0.8 and a minimum energy of 0.5. On the other hand, "Indie Folk" has the songs with the least energy with a mean energy of 0.4 and a minimum energy of 0.06. Lastly, "Indie Pop" has its songs' energies spread out the most with a mean of 0.6, a maximum energy of 0.9, and a minimum energy of 0.1. As an indie listener, this difference in energy is not surprising. Indie folk has a calm and mellow feel and usually uses light acoustic instruments, while indie rock can be louder with a faster tempo and more energetic instruments, such as the drumbs and the electric guitar. Indie pop can have both a calm and energetic feel when listening to it, for example the song "j's lullaby" by Delaney Bailey is very slow and quiet, while the song "I'm In Love With You" by The 1975 is much more upbeat.



### Indie Rock Has The Energy, Indie Folk Is For The Feels, Indie Pop Is Undecided {data-commentary-width=400}
```{r}
energy_valence <- ggplot(indie, aes(x = energy, y = loudness, color=valence, label = track.name)) + geom_point() + facet_wrap(~category) + scale_color_gradient(low = "yellow", high = "purple") + labs(x = "Energy", y = "Loudness") + geom_smooth(method=lm)
ggplotly(energy_valence)
```

***

This graph shows a consistent trend in all three sub-genres: as loudness increases, so does energy. Consistent with the last histogram, energy and loudness are widespread over the Indie genre as a whole. What can be observed again is that Spotify's "Indie Rock" playlist holds the songs with the most energy and loudness, followed by "Indie Pop" and "Indie Folk." Hence, the energy and loudness features can help us distinguish Indie songs from each other and categorize them into sub-genres. 

Indie folk separates itself from the other two sub-genres through the valence feature. Valence is measured from 0.0 to 1.0 and describes the musical positivity of a track. A song with a high valence (purple in this graph) is described as sounding happy, while a track with a low valence (yellow) sounds sad. Compared to rock and pop, most of the indie folk songs have a negative valence. In other words, Indie Folk is a sad sub-genre.

The song "j's lullaby" by Delaney Bailey is a clear outlier in this graph. It has the least energy and loudness by far compared to not only the other indie pop songs but also compared to the songs in the other two sub-genres. It is indeed a very mellow and calm song. Another atypical song is "Naked as We Came" by Iron & Wine since it is one of the few songs that is low in energy and loudness but has a positive valence. These specific songs are analyzed further in the "Comparing Pitch" tab.


### Indie pop lives up to its name, kind of
``` {r}
dance_pop <- indie |>                    # Start with awards.
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  ) |> ggplot(aes(x = track.popularity, y = danceability, color=mode, label = track.name)) + 
  geom_point() + facet_wrap(~category) + labs(x = "Danceability", y = "Popularity", color = "Mode") +
  scale_color_manual(values = c("#00AFBB", "#E7B800"))

ggplotly(dance_pop)
```

***



Comparing Pitch {data-navmenu="A Closer Look"}
==================================

Column {data-width=700}
-----------------------------------
### Energetic Indie Has A More Colorful Pitch Than Quiet Indie
```{r}
j_lullaby <-
  get_tidy_audio_analysis("7LNm1oWd6PniO9wRYpNY5o") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

j_lullaby_plot <- j_lullaby |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "\"j's lullaby\" by Delaney Bailey", subtitle = "Indie Pop, Lowest Energy, Low Valence") +
  theme_minimal() +
  scale_fill_viridis_c() + theme(plot.title = element_text(face="bold", size=8), plot.subtitle = element_text(size=6), axis.title = element_text(size=6), axis.text = element_text(size=6) , legend.title = element_text(size=6), legend.key.size = unit(0.3, 'cm'), legend.text = element_text(size=5))

live_once <-
  get_tidy_audio_analysis("4fPBB44eDH71YohayI4eKV") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

live_once_plot <- live_once |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "\"You Only Live Once\" by The Strokes", subtitle = "Indie Rock, Highest Energy, High Valence") +
  theme_minimal() +
  scale_fill_viridis_c() + theme(plot.title = element_text(face="bold", size=8), plot.subtitle = element_text(size=6), axis.title = element_text(size=6), axis.text = element_text(size=6) , legend.title = element_text(size=6), legend.key.size = unit(0.3, 'cm'), legend.text = element_text(size=5))

naked <- get_tidy_audio_analysis("4FuBIkfhZMoRgrDiHL6TYG") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

naked_plot <- naked |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "\"Naked As We Came\" by Iron & Wine", subtitle = "Indie Folk, Low Energy, High Valence") +
  theme_minimal() +
  scale_fill_viridis_c() + theme(plot.title = element_text(face="bold", size=8), plot.subtitle = element_text(size=6), axis.title = element_text(size=6), axis.text = element_text(size=6) , legend.title = element_text(size=6), legend.key.size = unit(0.3, 'cm'), legend.text = element_text(size=5))

wolf <- get_tidy_audio_analysis("03wKMRNYVvw6s9nm4I4jUS") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

wolf_plot <- wolf |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "\"Wolf Like Me\" by TV On The Radio", subtitle = "Indie Rock, High Energy, Low Valence") +
  theme_minimal() +
  scale_fill_viridis_c() + theme(plot.title = element_text(face="bold", size=8), plot.subtitle = element_text(size=6), axis.title = element_text(size=6), axis.text = element_text(size=6) , legend.title = element_text(size=6), legend.key.size = unit(0.3, 'cm'), legend.text = element_text(size=5))

grid.arrange(j_lullaby_plot, live_once_plot, naked_plot, wolf_plot, ncol=2)
```

Column {data-width=400 .tabset}
-----------------------------------
### Chromagrams
A chromagram is a visual representation of the pitch of a piece of music. It shows how much energy is contained in each pitch class at any given moment. These chromagrams of four different songs are interesting to compare, as each song has a different extreme level of valence and energy.

As discussed earlier, "j's lullaby" by Delaney Bailey is the song in the corpus with the lowest energy and valence. On the opposite extreme, "You Only Live Once" by The Strokes is one of the songs with the highest energy and highest valence. The other two songs displayed have an atypical combination of opposing energy and valences. The Indie Folk song "Naked As We Came" has low energy and high valence, while "Wolk Like Me" has high energy and low valence.

When comparing these four chromagrams at first glance, we can see that the two songs with high energy, "You Only Live Once" and "Wolf Like Me," have their signal's energy more widespread over all the pitch classes. Overall, their chromagrams have hints of yellow in almost every pitch class, while "j's lullaby" and "Naked As We Came" chromograms have their energy more contained in specific pitch classes. This may be because of the instruments played and the tuning of the instruments. "j's lullaby" and "Naked As We Came" only have the acoustic guitar playing and vocals, while "You Only Live Once" and "Wolf Like Me" use the electric guitar and drums which could lead to a more noisy chromagram.


### Individual Song Analysis
"j's lullaby" has most of its energy focused in pitch class A-flat, and some of its energy in D-flat. When listening to the song, the A-flat and D-flat major keys give the song a calm, sad, nostalgic tone, which explains the negative valence measure.

"Naked As We Came" has most of its energy contained in pitch classes E and B. The song is indeed written in an E major key. Although the song is low in energy and has a calm tone to it, the E major key gives the song a joyful mood. The song is structured as follows: Verse 1, Chorus, Verse 2, Chorus. Verses 1 and 2 are sung by a male vocalist, and a female voice joins at a different pitch during the chorus. In the chromagram, we observe this pitch change for both choruses: at around 45 seconds the energy moves from pitch class E to F-sharp, and then moves back to E at 65 seconds for Verse 2, this happens again 100 seconds into the song for the second chorus. 

"You Only Live Once" has most of its energy contained in pitch classes E and B. During the first 20 seconds, the song's energy is mostly contained in E, and then it splits alternating between E and B. At 160 seconds, the song abruptly moves all of its energy contained in pitch class B. This shift in pitch marks the last 30 seconds of the song characterized by a build up of energy. This Strokes song is written in B major, which helps set the harsh and angry tone the song projects. The E pitch class that was detected by Spotify, which is usually characterized by a happy tone, could indicate why the valence of the song is positive.

"Wolf Like Me" has most of its energy contained in pitch class B with some alternation to pitch G. Because the song is written in B major, which has an angry tone, this could be a reason for Spotify's valence measure being low, despite being high in energy.

Overall, the four songs do not have too much pitch variation, with most of their energies contained in one or two pitch classes. This could be an indicator of the "Indie" genre. As an indie listener, I would agree with this observation as most indie songs are written based on one key and do not deviate much from it.


Song Structures {.storyboard data-navmenu="A Closer Look"}
==================================

### Typical Indie Rock: Simple Chroma & Complex Timbre {data-width=600 data-commentary-width=500}
``` {r}
a_punk <-
  get_tidy_audio_analysis("3AydAydLzyyZutA0375XIz") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
          compmus_summarise, timbre,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

a_punk_chroma <- a_punk |>
  compmus_self_similarity(pitches, "cosine") |> # change timbre/pitches
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "", title = "Chroma-based Self-Similarity Matrix") + theme(title = element_text(size = 8))

a_punk_timbre <- a_punk |>
  compmus_self_similarity(timbre, "cosine") |> # change timbre/pitches
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "", title = "Timbre-Based Self-Similarity Matrix") + theme(title = element_text(size = 8))

grid.arrange(top = "\"A-Punk\" by Vampire Weekend", a_punk_chroma, a_punk_timbre, nrow=1)
```

***
#### What is a Self-Similarity Matrix?
A self-similarity matrix shows the degree of similarity of a specific feature of a song during any given time of a musical piece. It helps us analyze the structure of a song through either its variations in pitch (chroma) and timbre (instrumentation).

#### How Are They Interpreted?
Darker regions indicates high similarity and brighter regions indicates contrast. Dark parallel diagonal lines indicates repetition, and novelty is indicated by bright yellow bars. A dark blocks indicates homogeneity in either the pitch or instrumentation throughout that section, while a bright block indicates varying pitch or instrumentation.

#### "A-Punk" Analysis
The Vampire Weekend is a popular Indie Rock band. There debut album "Vampire Weekend" was independently released in 2008, hence officially falling under the "Indie" label. "A-Punk" is an energetic and youthful song that starts with a light-hearted guitar riff which is joined by drums and vocals.

The song has the following structure: verse, chorus, refrain, verse, chorus, refrain. In the chroma-based matrix, we can observe this pattern clearly through the 4 blocks along the diagonal: 0-40s is verse 1, 40-60s is chorus & refrain, 60-100s is verse 2, and then back to chorus and refrain. Verses 1 and 2 have more variety in the use of pitch since those blocks are brighter compared to the blocks representing the chorus and refrain. The chroma-based matrix helps us see this clear structure of the song. This pattern is most likely why the song has become so popular. The simple structure makes the song simple to follow and fun to dance to.

The timbre-based matrix is more interesting to look at. The first 5 seconds of the song begins with the guitar playing solo. At the 5 second mark, we can observe a bright bar emerging, which illustrates drums joining in. From then up to 40 seconds the instrumentation remains the same. At 40 seconds, the first change in timbre is marked by the addition of a flute, giving the song an extra layer of timbre complexity. At 60 seconds, the cymbal is abruptly played alone, marked by the bright bar seen on the graph. This is followed by a vocal "hey hey hey" with no instrumental background, marked by a dark section in the matrix, and then the drums come back in to move onto the next verse. This is the part of the song that gives the listener anticipation for what is coming next. After the second verse, the flute comes back followed by the "hey hey hey." At the end of the song (~130s), the cymbal comes back abruptly alone marking the end of the song. 

This popular indie rock song demonstrates that consistent pitch allows the song to be catchy and easy to follow, but the variation in timbre keeps the listener hooked. The instruments of the indie rock bands play an important role in creating an interesting and successful indie rock song.


### Typical Indie Folk: Repeating Chroma & Simple Timbre {data-commentary-width=400}
``` {r}
flightless <-
  get_tidy_audio_analysis("1fEGtTZjrjJW8eUeewnNJR") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
          compmus_summarise, timbre,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

flightless_chroma <- flightless |>
  compmus_self_similarity(pitches, "cosine") |> # change timbre/pitches
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "", title = "Chroma-based Self-Similarity Matrix") + theme(title = element_text(size = 8))

flightless_timbre <- flightless |>
  compmus_self_similarity(timbre, "cosine") |> # change timbre/pitches
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "", title = "Timbre-Based Self-Similarity Matrix") + theme(title = element_text(size = 8))

grid.arrange(top="\"Flightless Bird, American Mouth\" by Iron & Wine", flightless_chroma, flightless_timbre, ncol=2)
```

*** 
Iron & Wine is an American indie folk singer-songwriter. His songs have the typical indie folk sound, which is confirmed by the fact that 2 out of the 50 songs in Spotify’s “Indie Folk” playlist are his. Although he is considered an indie artist, he has never been an independent artist. His first album “The Creek Drank the Cradle” was released in 2002 on the Sub Pop label. Hence, “Flightless Bird, American Mouth” by Iron & Wine is an interesting song to analyze as it is labeled an indie song, but was not independently released.

The self-similarity matrices we see on the left are to be expected. Indie folk songs are characterized by a very simple structure known as a strophic form, a particular sequence is repeated over and over in the song. In this particular song, a verse is repeated four times with the same melody, but differing lyrics. 

When considering timbre, the only two instruments used during the song are the guitar and tambourine. The timbre-based matrix mostly contains dark blocks, which demonstrates the homogeneity of instrumentation. The thin bright lines simply indicate that one instrument is not being played anymore or another is added. These lines help us discern the four verses through four homogeneous blocks along the diagonal.

The chroma-based self-similarity matrix shows the repeating use of pitch, throughout the song. As described earlier, the dark diagonal lines indicate repetition. Hence, a particular section with varying pitch is repeated over and over again throughout the song. 

In summary, an indie folk song is characterized by homogeneous instrumentation and repeating pitch.



### Typical Indie Pop: Simple Chroma & Simple Timbre {data-commentary-width=400}
``` {r}
cielings <-
  get_tidy_audio_analysis("2L9N0zZnd37dwF0clgxMGI") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
          compmus_summarise, timbre,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

cielings_chroma <- cielings |>
  compmus_self_similarity(pitches, "cosine") |> # change timbre/pitches
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "", title = "Chroma-based Self-Similarity Matrix") + theme(title = element_text(size = 8))

cielings_timbre <- cielings |>
  compmus_self_similarity(timbre, "cosine") |> # change timbre/pitches
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "", title = "Timbre-Based Self-Similarity Matrix") + theme(title = element_text(size = 8))

grid.arrange(top="\"Cielings\" by Lizzie McAlpine", cielings_chroma, cielings_timbre, ncol=2)
```

*** 
“Ceilings” by Lizzie McAlpine is the most popular song in the “Indie Pop” playlist. She Hence, it is an interesting song to analyze. 

Starts with guitar (20s)


Indie Keys and Chords {.storyboard data-navmenu="A Closer Look"}
=====================================

### Indie Keys 
```{r}
key_histogram <- ggplot(indie, aes(x=key, color=category,fill=category)) + geom_histogram(binwidth = 1, alpha=0.6) + scale_x_continuous(breaks = c(0:11), labels=c("C","C#","D","D#","E","F","F#","G","G#","A","A#","B")) + 
  facet_wrap(~category) + labs(y = "Count", x="Key", title = "Comparing the Distribution of Key Between Indie Subgenres")
ggplotly(key_histogram)
```
***
This histogram shows the distribution of keys in each indie subgenre playlist. The most common used keys in Indie Pop are C and C#. This is to be expected, as C and G major are the most common key used in pop music. Although G major is one of the most common keys used in pop music, G is surprisingly the least common key in Indie Pop. Hence, indie pop follows the pop-genre norm when it comes to having songs with C as a key, but the minority of the songs are in the key of G. The other two indie genres have a less distinct majority key. Indie Folk has the majority of its songs in the keys C, D, E, and F, and the minority in keys G# and A#. Indie Rock has the majority of its songs in the keys C, D, G, and A, and the minority in keys D#, G#, and A#. C is thus the most common key used in all three genres, followed by D. Overall, the keys used in each subgenre are widespread, with some more and others less common.


### "White Winter Hymnal" by Fleet Foxes {data-commentary-width=500}
``` {r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

# 0GegHVxeozw3rdjte45Bfx - winter_hymnal - chord_templates interesting

winter_hymnal <-
  get_tidy_audio_analysis("0GegHVxeozw3rdjte45Bfx") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan"
      )
  )


winter_hymnal |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if desired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "", title = "Chordogram of \"White Winter Hymnal\" by Fleet Floxes")

```

*** 
A chordogram is a visual representation of the different chords used in a song at any given time. These are made by comparing the chords in the audio recording of the song to a template of predefined chord models. The darker sections in the chordogram are, the more similar those chords are to the template, meaning this chord is being played at a particular time.

"White Winter Hymnal" by Fleet Foxes is another typical indie folk song. It is very repetitive in its melody and lyrics. Its structure consists of an intro followed by the chorus three times. This is very characteristic of indie folk music. The only thing that truly changes throughout the song is the chords used throughout. This chordogram shows that "White Winter Hymnal" uses distinct chords during distinct sections of the song. From 0 to 30s, the song is in an E7 chord, this is the intro of the song. From 30 to 40s, the song goes down to an A7 chord, marking a transition between the intro and the first chorus. From 40 to 60s, the chord goes back to E7 for the chorus. From 60 to 70s, the song goes further down to an F# minor, marking another transtion from the first chorus to the second chorus, and so on. What is interesting to see is that the band marking the chord G-flat 7 is dark throughout the whole chordogram. This is the base chord the song is centered around and then the variation throughout the song indicates the different sections of the song.


Exploring Tempo {.storyboard data-navmenu="A Closer Look"}
=====================================

### Indie Rock is the Fastest, Folk is more Relaxed {data-commentary-width=400}
```{r}

temp <- indie %>%
  group_by(category) %>%
  summarise(temp_mean = mean(tempo))

tempo_hist <- ggplot(indie, aes(x=tempo, color=category,fill=category)) +
  geom_histogram(binwidth = 10, alpha=0.6) +
  geom_density() +
  labs(y = "Count", x="Tempo", title = "Comparing the Distribution of Tempo Between Indie Subgenres") +
  geom_vline(temp, mapping = aes(xintercept = temp_mean),col="purple") + 
  facet_wrap(~category)

ggplotly(tempo_hist)
```

***
Spotify’s tempo feature measures the average tempo of a track. Most popular songs today have a tempo between 100 and 140bpm. This histogram shows that the Indie genre tempo generally lies between 70 and 200bpm, which is overlaps the popular tempo. The means of all three playlists, which are represented by the purple line in the graph, are close to 120bpm. Indie rock has the fastest tempo with an average of 127bpm, followed by pop with 124bpm, and lastly folk has slowest mean tempo of 116bpm. A tempo between 109-132bpm is described as fast, quick, and bright. This describes the tempo of most of the Indie songs in the corpus. 

Indie Folk is a slower form of the genre as its mode is centered at 100bpm; 20% of the songs in this playlist have a tempo of 100bpm. Furthermore, three songs in this playlist have a tempo of 70bpm, while the lowest tempo in the other two playlists is 80bpm. Indie folk having a slower tempo is to be expected as a listener, because indie folk does indeed sound slower-paced.

Indie rock and pop have similar tempo distributions. Both their modes are centered around 120bpm, with 20% of their songs having this tempo. This falls right in the middle of the range of tempo of popular songs today.
Overall, however, the tempo of all three sub-genres is spread out in a similar manner.


### Indie Songs Stick To Their Chosen Tempo {data-commentary-width=400}
``` {r}
volume_tempo <- indie |>
  mutate(
    sections =
      map(
        sections,                                    # sections or segments
        summarise_at,
        vars(tempo, loudness, duration),             # features of interest
        list(section_mean = mean, section_sd = sd)   # aggregation functions
      )
  ) |>
  unnest(sections) |>
  ggplot(
    aes(
      x = tempo,
      y = tempo_section_sd,
      colour = category,
      alpha = loudness
    )
  ) +
  geom_point(aes(size = duration / 60, alpha = loudness)) +
  geom_rug() +
  theme_minimal() +
  ylim(0, 5) +
  labs(
    x = "Mean Tempo (bpm)",
    y = "SD Tempo",
    colour = "Genre",
    size = "Duration (min)",
    alpha = "Volume (dBFS)"
  )

volume_tempo
```

***
This graph displays information about the overall temporal features of the three playlists. The duration of a song is represented by a point's size, and the volume is represented by point transparency. As discussed before, we can observe that indie folk is the quietest sub-genre with many orange points being more transparent compared to points from other sub-genres. The duration of the song vary across each sub-genre in similar ways. Hence, the duration of songs cannot help us distinguish between the three playlists, which can vary between 2mins and 6mins.

This graph plots the average tempo of a song in BPM and the standard deviation of the tempo within the song itself. In this case, the standard deviation is a measure of variation in tempo in a song in BPM. Most of the songs are clustered at a standard deviation below 1, indicating that most indie songs stick to one tempo throughout the entire song. Furthermore, all the songs have a standard deviation below 5BPM, which is a variation in tempo that humans are not able to perceive and could therefore simply be due to errors in audio analysis. This consistent tempo is typical in indie music, and this graph confirms that the indie genre does not play much with tempo variation.


Zooming Back Out {.storyboard}
======================================
### Can Spotify's Audio Features Group Indie Sub-Genres? {data-commentary-width=400}
``` {r}
indie <-
  bind_rows(
    folk |> mutate(category = "Indie Folk", playlist_name = "Indie Folk") |> slice_head(n = 30),
    pop |> mutate(category = "Indie Pop", playlist_name = "Indie Pop") |> slice_head(n = 30),
    rock |> mutate(category= "Indie Rock", playlist_name = "Indie Rock") |> slice_head(n = 30)
  ) |> 
  add_audio_analysis()


# Note that you are not allowed to have duplicate songs in the dataset! 
indie |>
  count(track.name) %>%
  arrange(desc(n))

# Grab only track names where it's listed more than once
tracks_to_remove <- indie |>
  count(track.name) |>
  filter(n >= 2) %>%
  select(track.name)

indie <- indie %>%
  anti_join(tracks_to_remove)


# Similar to code from class, still trying to predict track.name but later we add on playlist name!
indie_juice <-
  recipe(
    track.name ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo,
    data = indie
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors()) |> 
  # step_range(all_predictors()) |> 
  prep(indie |> mutate(track.name = str_trunc(track.name, 20))) |>
  juice() |>
  column_to_rownames("track.name")

indie_dist <- dist(indie_juice, method = "euclidean")

data_for_indie_clustering <- indie_dist |> 
  hclust(method = "average") |> # average for a balanced tree!
  dendro_data() 

playlist_data_for_join <- indie %>%
  select(track.name, playlist_name) %>%
  mutate(label = str_trunc(track.name, 20))

data_for_indie_clustering$labels <- data_for_indie_clustering$labels %>%
  left_join(playlist_data_for_join)

# Add factor so can use colouring! 
data_for_indie_clustering$labels$label <- factor(data_for_indie_clustering$labels$label)


data_for_indie_clustering |>
  ggdendrogram() +
  geom_text(data = label(data_for_indie_clustering), aes(x, y, 
                                                         label=label, 
                                                         hjust=0, 
                                                         colour=playlist_name), size=3) +
  coord_flip() + 
  scale_y_reverse(expand=c(0.2, 0)) +
  theme(axis.line.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.text.y=element_blank(),
        axis.title.y=element_blank(),
        panel.background=element_rect(fill="white"),
        panel.grid=element_blank()) +
  labs(title = "Playlist Clustering") +
  guides(
    colour = guide_legend(
      title = "Playlist"
    )
  )
```

***
This graph corresponds to hierarchical clustering, which relies on unsupervised machine learning whereby unlabeled songs are grouped together based on their similarity in Spotify's audio feature measurements. In this cluster, the track names are written at the bottom of each branch, and the color corresponds to which playlist they belong to. The songs that are most similar to each other would be closer together at the end of the tree. Here we can observe that through Spotify's audio features, the songs in the "Indie Folk" playlist are clustered together, with most of the songs on top being colored in red. The songs in the "Indie Rock" playlist are also clearly grouped together in blue. This means that Spotify is able to use its audio features to distinguish the two indie sub-genres from each other. In contrast, the "Indie Pop" playlist does not have an obvious cluster. This is consistent with indie pop being spread out in energy and valence which we observed earlier. This could be because indie pop has a less recognizable "sound" to it. Wikipedia describes indie pop as "a music genre and subculture that combines guitar pop with DIY ethic in opposition to the style and tone of mainstream pop music." This "DIY ethic" could be the factor that distinguishes indie pop songs from each other. Artists could have a different spin to a pop sound, making each indie pop song sound slightly different and vary within Spotify's audio features. 


### How accurate is the grouping? {data-commentary-width=400}
``` {r}
folk <- get_playlist_audio_features("", "6P82OCSEpo1VrZ4TZ2W5jK")
pop <- get_playlist_audio_features("", "3HDdTtyB76aLDc4wpGFCX4")
rock <- get_playlist_audio_features("","5dMoFfrgm8aOOkneQjt6LT")

indie <-
  bind_rows(
    pop |> mutate(playlist = "Indie Pop") |> slice_head(n = 30),
    folk |> mutate(playlist = "Indie Folk") |> slice_head(n = 30),
    rock |> mutate(playlist = "Indie Rock") |> slice_head(n = 30)
  ) |> add_audio_analysis()

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
} 

indie_features <-
  indie |>  # For your portfolio, change this to the name of your corpus.
  mutate(
    playlist = factor(playlist),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))

indie_recipe <-
  recipe(
    playlist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = indie_features           # Use the same name as the previous block.
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors())

indie_cv <- indie_features |> vfold_cv(5)

knn_model <-
  nearest_neighbor(neighbors = 1) |>
  set_mode("classification") |> 
  set_engine("kknn")
indie_knn <- 
  workflow() |> 
  add_recipe(indie_recipe) |> 
  add_model(knn_model) |> 
  fit_resamples(indie_cv, control = control_resamples(save_pred = TRUE))

indie_knn |> get_conf_mat() |> autoplot(type = "mosaic")
```

*** 
This graph is a classification cross-validation graph. Classification relies on supervised machine learning. In this context, classification predicts which genre songs belong to based on Spotify's audio features. This prediction is then compared to the actual genre the song belongs to in cross-validation. The area of the boxes in this graph correspond to the number of songs that belong to a certain genre based on both prediction and truth. The graph shows that classification using Spotify's audio features is mostly accurate. The prediction has the majority of the songs grouped in the correct sub-genre. About a quarter of the indie folk songs were predicted to be indie pop, a quarter of the indie pop songs were predicted to be folk, and another quarter of the pop songs were predicted to be rock songs. This further shows the variation in the indie pop songs. Because indie pop does not have a distinct sound from the other two sub-genres, the algorithm has a harder time predicting the correct sub-genre. In contrast, indie rock songs are predicted with approximately 80% accuracy and are more easily distinguished. 


### Which features are most important for distinguishing the three sub-genres? {data-commentary-width=400}
``` {r}
forest_model <-
  rand_forest() |>
  set_mode("classification") |> 
  set_engine("ranger", importance = "impurity")
indie_forest <- 
  workflow() |> 
  add_recipe(indie_recipe) |> 
  add_model(forest_model) |> 
  fit_resamples(
    indie_cv, 
    control = control_resamples(save_pred = TRUE)
  )

workflow() |> 
  add_recipe(indie_recipe) |> 
  add_model(forest_model) |> 
  fit(indie_features) |> 
  pluck("fit", "fit", "fit") |>
  ranger::importance() |> 
  enframe() |> 
  mutate(name = fct_reorder(name, value)) |> 
  ggplot(aes(name, value)) + 
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  labs(x = NULL, y = "Importance", title = "Random Forrest of Indie Sub-Genre Prediction")
```

***
This random forrest uses the cross-validation data discussed previously to give us a ranking of feature importance. It measures how useful each Spotify audio feature was for distinguishing the three indie sub-genres. Here we can see that acousticness was the most helpful feature, followed by timbre component 1 and timbre component 2. Timbre refers to the quality of a musical sound that distinguishes different types of musical instruments, or voices. Timbre component 1 refers to the loudness of a song, but, since we already analyzed loudness previously, we will focus on analyzing timbre component 2, which measures a sound's brightness. The next page will further discuss how acousticness and brightness differ between the three playlists.


### Indie Folk is Acoustic, Indie Rock is the Brightest, Indie Pop Enjoys Variety {data-commentary-width=400}
``` {r}
indie_features |>
  ggplot(aes(x = acousticness, y = c02, colour = playlist, size = energy)) +
  geom_point(alpha = 0.8) +
  scale_color_viridis_d() +
  labs(
    x = "Acousticness",
    y = "Timbre Component 2 (Brightness)",
    size = "Energy",
    colour = "Playlist",
    title = "Brightness, Acousticness, and Energy"
  )
```

***
This graph plots timbre component 2 against acousticness, with the size of the points correspond to the song's energy level. A general trend that is observed is as acousticness increases, brightness decreases. With brightness being negatively correlated with acousticness, we can assume that an acoustic guitar is not considered to be bright according to Spotify's audio features. Indie rock is clearly recognized as being energetic and having a bright timbre sound. This could be because of the electric guitar usually used in indie rock. Another observation we can make is that indie folk is an acoustic sub-genre. This is not surprising at all because most indie folk songs are indeed acoustic. Lastly, consistent with the previous analyses, indie pop has songs with varying amounts of energy, brightness, and acousticness.




